{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elotl.nahuatl as nahuatl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load an english to spanish transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planta de coyote\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "nllb = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "eng_text = \"coyote plant\"\n",
    "inputs = nllb_tokenizer(f\"eng_Latn {eng_text}\", return_tensors=\"pt\")\n",
    "translated = nllb.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(\"spa_Latn\"),\n",
    "    max_length=30\n",
    ")\n",
    "spanish_text = nllb_tokenizer.batch_decode(\n",
    "    translated, skip_special_tokens=True)[0]\n",
    "print(spanish_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load spanish to nahuatl model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import  AutoModelForSeq2SeqLM\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"somosnlp-hackathon-2022/t5-small-spanish-nahuatl\")\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"somosnlp-hackathon-2022/t5-small-spanish-nahuatl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   3, 157,  32,  63,  32,  17,  40,   3,  17,  40, 144,  40, 138,\n",
      "         173,  23,   1]])\n",
      "koyotl tlatlalili\n"
     ]
    }
   ],
   "source": [
    "input_ids = t5_tokenizer('translate Spanish to Nahuatl: ' +\n",
    "                          spanish_text, return_tensors='pt').input_ids\n",
    "outputs = t5_model.generate(input_ids)\n",
    "print(outputs)\n",
    "decoded_output = t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)[\n",
    "    0]  # Decode the tokens to text\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok not great but kind of expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Aca-capac-quili-tl\n",
      "1                        Aca-tl\n",
      "2                     A-chil-li\n",
      "3      Acocotli; Acoco-xihui-tl\n",
      "4                     Acxoya-tl\n",
      "                 ...           \n",
      "305                    coco-tli\n",
      "306                    pezo-tli\n",
      "307              acuecueyalo-tl\n",
      "308                      oc-tli\n",
      "309                tla-qua-tzin\n",
      "Name: nahuatl, Length: 310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_text_files, load_subchapter_data\n",
    "\n",
    "file_names, subchapter_names, texts = load_text_files()\n",
    "subchapter_data = load_subchapter_data()\n",
    "\n",
    "nahuatl_texts = subchapter_data[\"nahuatl\"]\n",
    "print(nahuatl_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Aca-capac-quili-tl\n",
      "1                        Aca-tl\n",
      "2                     A-chil-li\n",
      "3      Acocotli; Acoco-xihui-tl\n",
      "4                     Acxoya-tl\n",
      "                 ...           \n",
      "305                    coco-tli\n",
      "306                    pezo-tli\n",
      "307              acuecueyalo-tl\n",
      "308                      oc-tli\n",
      "309                tla-qua-tzin\n",
      "Name: nahuatl, Length: 310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def segment_nahuatl(text):\n",
    "    \"\"\"Segments Nahuatl text into morphemes using common patterns\"\"\"\n",
    "    suffixes = ['tzin', 'tli', 'tin', 'que', 'pan', 'can', 'tic', 'toc',\n",
    "                'tia', 'lia', 'hui', 'tla', 'tli', 'tl', 'li', 'in']\n",
    "    prefixes = ['ni', 'ti', 'xi', 'mo', 'no', 'to', 'ne', 'te', 'tla', 'on']\n",
    "\n",
    "    words = str(text).split()\n",
    "    segmented_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) < 3:\n",
    "            segmented_words.append(word)\n",
    "            continue\n",
    "\n",
    "        segmented = word\n",
    "        for suffix in suffixes:\n",
    "            if segmented.endswith(suffix):\n",
    "                segmented = segmented[:-len(suffix)] + '-' + suffix\n",
    "                break\n",
    "\n",
    "        for prefix in prefixes:\n",
    "            if segmented.startswith(prefix):\n",
    "                segmented = prefix + '-' + segmented[len(prefix):]\n",
    "                break\n",
    "\n",
    "        segmented_words.append(segmented)\n",
    "\n",
    "    return ' '.join(segmented_words)\n",
    "\n",
    "\n",
    "def load_segmented_subchapter_data(filepath=\"./verify_subchapter.csv\"):\n",
    "    \"\"\"Loads and processes subchapter data from CSV, returning relevant columns\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    subchapter = df[\"subchapter\"].str.split(\";\", expand=True).fillna(0)\n",
    "\n",
    "    nahuatl = df[\"nahuatl\"].apply(segment_nahuatl)\n",
    "\n",
    "    return {\n",
    "        'subchapter': subchapter,\n",
    "        'nahuatl': nahuatl,\n",
    "        'official_name': df[\"official_name\"],\n",
    "        'ID': df[\"ID\"],\n",
    "        'type': df[\"type\"]\n",
    "    }\n",
    "\n",
    "file_names, subchapter_names, texts = load_text_files()\n",
    "segmented_subchapter_data = load_segmented_subchapter_data()\n",
    "\n",
    "regex_nahuatl_texts = segmented_subchapter_data[\"nahuatl\"]\n",
    "print(regex_nahuatl_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar to tlal:\n",
      "matlal: 0.3658\n",
      "Matlal: 0.3502\n",
      "tla: 0.3403\n",
      "\n",
      "Similar to xochi:\n",
      "xochitI: 0.5757\n",
      "xochiti: 0.4645\n",
      "can: 0.3305\n",
      "\n",
      "Similar to tl:\n",
      "tli: 0.3892\n",
      "tlemai: 0.2621\n",
      "Coz: 0.2607\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = []\n",
    "for text in regex_nahuatl_texts:\n",
    "    morphemes = [morpheme for word in text.split()\n",
    "                 for morpheme in word.split('-')]\n",
    "    sentences.append(morphemes)\n",
    "\n",
    "model = FastText(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    min_n=3,\n",
    "    max_n=6,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save(\"nahuatl_morpheme_embeddings.bin\")\n",
    "\n",
    "test_morphemes = [\"tlal\", \"xochi\", \"tl\"]\n",
    "for morpheme in test_morphemes:\n",
    "    if morpheme in model.wv:\n",
    "        print(f\"\\nSimilar to {morpheme}:\")\n",
    "        similar = model.wv.most_similar(morpheme)\n",
    "        for word, score in similar[:3]:\n",
    "            print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xochitI', 0.5757061243057251), ('xochiti', 0.4644985496997833), ('can', 0.33054521679878235), ('CÃ³lto', 0.2992047667503357), ('Eca', 0.2878001928329468)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"xochi\", topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpelotl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
